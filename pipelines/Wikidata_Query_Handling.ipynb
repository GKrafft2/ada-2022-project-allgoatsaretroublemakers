{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work book is going to handle all of the data that we obtained from wikidata which is the following\n",
    "\n",
    "- Actor Data and names\n",
    "- Film Actor Data and names\n",
    "- Television Actor Data and names\n",
    "- Voice Actor Data and names\n",
    "- Director Data and names\n",
    "- Film Director Data and names\n",
    "- Screenwriter Data and names\n",
    "\n",
    "The data collected is as follows\n",
    "\n",
    "- Name\n",
    "- Gender\n",
    "- Date of Birth\n",
    "- Height\n",
    "- Place of birth\n",
    "- Ethnic group\n",
    "- Citizenship\n",
    "- Eye colour\n",
    "- Hair colour\n",
    "\n",
    "First step lets read all of the csv's with data and begin to group them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliwa\\AppData\\Local\\Temp\\ipykernel_12284\\3059737156.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  actor = pd.read_csv('./Wikidata_queries/data_full/actor_data_full.csv')\n",
      "C:\\Users\\aliwa\\AppData\\Local\\Temp\\ipykernel_12284\\3059737156.py:7: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  swriter = pd.read_csv('./Wikidata_queries/data_full/screenwriter_data_full.csv')\n"
     ]
    }
   ],
   "source": [
    "actor = pd.read_csv('./Wikidata_queries/data_full/actor_data_full.csv')\n",
    "film_actor = pd.read_csv('./Wikidata_queries/data_full/film_actor_data_full.csv')\n",
    "tv_actor = pd.read_csv('./Wikidata_queries/data_full/television_actor_data_full.csv')\n",
    "voice_actor = pd.read_csv('./Wikidata_queries/data_full/voice_actor_data_full.csv')\n",
    "director = pd.read_csv('./Wikidata_queries/data_full/director_data_full.csv')\n",
    "film_director = pd.read_csv('./Wikidata_queries/data_full/film_director_data_full.csv')\n",
    "swriter = pd.read_csv('./Wikidata_queries/data_full/screenwriter_data_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by combining all of the actor datasets together and the director/screenwriter datasets together. There are likely to be duplicates and if we find those we can drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor\n",
      "television_actor\n",
      "voice_actor\n",
      "film_actor\n",
      "film_director\n",
      "director\n",
      "screenwriter\n"
     ]
    }
   ],
   "source": [
    "print(actor.columns[0])\n",
    "print(tv_actor.columns[0])\n",
    "print(voice_actor.columns[0])\n",
    "print(film_actor.columns[0])\n",
    "print(film_director.columns[0])\n",
    "print(director.columns[0])\n",
    "print(swriter.columns[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the first column name so that they are all the same\n",
    "film_actor.rename(columns = {'film_actor':'actor'}, inplace=True)\n",
    "tv_actor.rename(columns = {'television_actor':'actor'}, inplace=True)\n",
    "voice_actor.rename(columns = {'voice_actor':'actor'}, inplace=True)\n",
    "film_director.rename(columns = {'film_director':'dir_swriter'}, inplace=True)\n",
    "director.rename(columns = {'director':'dir_swriter'}, inplace=True)\n",
    "swriter.rename(columns = {'screenwriter':'dir_swriter'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "actor_datasets = [actor,film_actor,tv_actor,voice_actor]\n",
    "director_datasets = [director,film_director,swriter]\n",
    "\n",
    "all_actors = pd.concat(actor_datasets)\n",
    "all_dir = pd.concat(director_datasets)\n",
    "\n",
    "#we are going to drop place of eye colour, hair colour, birth place, and citizenship data because we are likely not going to use it for our purposes\n",
    "all_actors.drop('place_of_birth', inplace=True, axis=1)\n",
    "all_actors.drop('citizenship', inplace=True, axis=1)\n",
    "all_actors.drop('eye_colour', inplace=True, axis=1)\n",
    "all_actors.drop('hair_colour', inplace=True, axis=1)\n",
    "\n",
    "all_dir.drop('place_of_birth', inplace=True, axis=1)\n",
    "all_dir.drop('citizenship', inplace=True, axis=1)\n",
    "all_dir.drop('eye_colour', inplace=True, axis=1)\n",
    "all_dir.drop('hair_colour', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['actor', 'gender', 'dob', 'height', 'ethnic_group'], dtype='object')\n",
      "Index(['dir_swriter', 'gender', 'dob', 'height', 'ethnic_group'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(all_actors.columns)\n",
    "print(all_dir.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's check how many duplicates there are in each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total amount of actor rows are 763194, the number of unique values are 395291\n",
      "The total amount of actor rows are 394761, the number of unique values are 179655\n"
     ]
    }
   ],
   "source": [
    "print(f'The total amount of actor rows are {len(all_actors)}, the number of unique values are {all_actors.actor.nunique()}')\n",
    "print(f'The total amount of actor rows are {len(all_dir)}, the number of unique values are {all_dir.dir_swriter.nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will jut drop all of the rows that are completely identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total amount of actor rows left are 403549, but the number of unique actors is still 395291\n",
      "The total amount of actor rows left are 182884, but the number of unique directors is still 179655\n"
     ]
    }
   ],
   "source": [
    "all_actors.drop_duplicates(keep='first', inplace=True)\n",
    "all_dir.drop_duplicates(keep='first', inplace=True)\n",
    "print(f'The total amount of actor rows left are {len(all_actors)}, but the number of unique actors is still {all_actors.actor.nunique()}')\n",
    "print(f'The total amount of actor rows left are {len(all_dir)}, but the number of unique directors is still {all_dir.dir_swriter.nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have made a lot of progress, but there is still data that needs to be cleaned so that we can have only one actor/director per row. To start we can start by grouping the objects by the actor wikidata page and then looking at the ones that have more than one value to understand what the differences are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['actor', 'gender', 'dob', 'height', 'ethnic_group'], dtype='object')\n",
      "Index(['dir_swriter', 'gender', 'dob', 'height', 'ethnic_group'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "all_actors.sort_values('actor',inplace=True)\n",
    "all_dir.sort_values('dir_swriter', inplace=True)\n",
    "dup_actors = all_actors[all_actors.actor.duplicated(keep=False)]\n",
    "dup_dirs = all_dir[all_dir.dir_swriter.duplicated(keep=False)]\n",
    "grouped_dup_actors = dup_actors.groupby('actor')\n",
    "grouped_dup_dirs = dup_dirs.groupby('dir_swriter')\n",
    "print(all_actors.columns)\n",
    "print(all_dir.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's clear up any issues arising from two different rows just not having the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterg = []\n",
    "counterd = []\n",
    "counterh = []\n",
    "countere = []\n",
    "for name, group in grouped_dup_actors:\n",
    "        gend = []\n",
    "        dob = [] \n",
    "        height = []\n",
    "        ethnicity = []\n",
    "\n",
    "        cols = [['gender',gend],['dob',dob],['height',height],['ethnic_group',ethnicity]]\n",
    "\n",
    "        for row in group.iterrows():\n",
    "                for col,lst in cols:\n",
    "                        if not pd.isna(row[1][col]):\n",
    "                                lst.append(row[1][col])\n",
    "        counterg.append(len(cols[0][1]))\n",
    "        counterd.append(len(cols[1][1]))\n",
    "        counterh.append(len(cols[2][1]))\n",
    "        countere.append(len(cols[3][1]))\n",
    "        for col, lst in cols:\n",
    "                if len(lst) == 1:\n",
    "                        print('we got one a')\n",
    "                        row[col] = lst[0]\n",
    "\n",
    "for name, group in grouped_dup_dirs:\n",
    "        gend = []\n",
    "        dob = [] \n",
    "        height = []\n",
    "        ethnicity = []\n",
    "\n",
    "        cols = [['gender',gend],['dob',dob],['height',height],['ethnic_group',ethnicity]]\n",
    "\n",
    "        for row in group.iterrows():\n",
    "                for col,lst in cols:\n",
    "                        if not pd.isna(row[1][col]):\n",
    "                                lst.append(row[1][col])\n",
    "        for col, lst in cols:\n",
    "                if len(lst) == 1:\n",
    "                        print('we got one b')\n",
    "                        row[1][col] = lst[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that there are no values in which one row has NaN and the other has a value in the actor's dataframe or in the directors data frame which means that we are dealing with actual duplicates which have conflicting data, to move forward with this without spending more time I am going to drop the duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total amount of actor data is 395291 and the amount of data to drop is 7904, which is 1.9995395797020423%\n",
      "The total amount of director data is 182884 and the amount of data to drop is 3054, which is 1.6999248559739502%\n"
     ]
    }
   ],
   "source": [
    "print(f'The total amount of actor data is {all_actors.actor.nunique()} and the amount of data to drop is {len(grouped_dup_actors)}, which is {100*len(grouped_dup_actors)/all_actors.actor.nunique()}%')\n",
    "print(f'The total amount of director data is {len(all_dir)} and the amount of data to drop is {len(grouped_dup_dirs)}, which is {100*len(grouped_dup_dirs)/all_dir.dir_swriter.nunique()}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we drop the data we can run some cleaning on the gender data since we only want to keep either Male or Female. We are also going to keep the NaN values because we have already verified that they are not conflicts with NaN values and we can still use that data to maybe verify other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before there was 395291 unique actor data and now there is 394530\n",
      "Before there was 179655 unique director data and now there is 179425\n",
      "The length of the actor list is now 402736\n",
      "The length of the director list is now 182623\n"
     ]
    }
   ],
   "source": [
    "all_actors_clean = all_actors.copy()\n",
    "all_dir_clean = all_dir.copy()\n",
    "\n",
    "male = 'http://www.wikidata.org/entity/Q6581097' \n",
    "female =  'http://www.wikidata.org/entity/Q6581072'\n",
    "\n",
    "all_actors_clean.drop(\n",
    "    all_actors_clean.loc[\n",
    "        (all_actors_clean['gender']!=male) & \n",
    "        (all_actors_clean['gender']!=female) &\n",
    "        (all_actors_clean.gender.notna())\n",
    "        ].index, inplace=True)\n",
    "\n",
    "all_dir_clean.drop(\n",
    "    all_dir_clean.loc[\n",
    "        (all_dir_clean['gender']!=male) & \n",
    "        (all_dir_clean['gender']!=female) &\n",
    "        (all_dir_clean.gender.notna())\n",
    "        ].index, inplace=True)\n",
    "\n",
    "print(f'Before there was {all_actors.actor.nunique()} unique actor data and now there is {all_actors_clean.actor.nunique()}')\n",
    "print(f'Before there was {all_dir.dir_swriter.nunique()} unique director data and now there is {all_dir_clean.dir_swriter.nunique()}')\n",
    "print(f'The length of the actor list is now {len(all_actors_clean)}')\n",
    "print(f'The length of the director list is now {len(all_dir_clean)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can drop the duplicates and start to join the naming lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new length of the actor list is 386668\n",
      "The new length of the directors list is 174323\n"
     ]
    }
   ],
   "source": [
    "all_actors_clean.drop(all_actors_clean.loc[all_actors_clean.actor.duplicated(keep=False)].index, inplace=True)\n",
    "all_dir_clean.drop(all_dir_clean.loc[all_dir_clean.dir_swriter.duplicated(keep=False)].index, inplace=True)\n",
    "\n",
    "print(f'The new length of the actor list is {len(all_actors_clean)}')\n",
    "print(f'The new length of the directors list is {len(all_dir_clean)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check that we now only have unique actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unqiue actor names are 386668\n",
      "The number of unqiue director names are 174323\n"
     ]
    }
   ],
   "source": [
    "print(f'The number of unqiue actor names are {all_actors_clean.actor.nunique()}')\n",
    "print(f'The number of unqiue director names are {all_dir_clean.dir_swriter.nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have dealt with that we can go ahead and gather the naming data for everything and then start to make joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_name_F = pd.read_csv('./Wikidata_queries/naming_lists/actor_name_female.csv')\n",
    "actor_name_M = pd.read_csv('./Wikidata_queries/naming_lists/actor_name_male.csv')\n",
    "film_actor_name = pd.read_csv('./Wikidata_queries/naming_lists/film_actor_names.csv')\n",
    "tv_actor_name = pd.read_csv('./Wikidata_queries/naming_lists/television_actor_names.csv')\n",
    "voice_actor_name = pd.read_csv('./Wikidata_queries/naming_lists/voice_actor_names.csv')\n",
    "\n",
    "dir_name = pd.read_csv('./Wikidata_queries/naming_lists/director_names.csv')\n",
    "film_dir_name = pd.read_csv('./Wikidata_queries/naming_lists/film_director_names.csv')\n",
    "swriter_name = pd.read_csv('./Wikidata_queries/naming_lists/screenwriter_names.csv')\n",
    "\n",
    "gender_name = pd.read_csv('./Wikidata_queries/naming_lists/gender_names.csv')\n",
    "ethn_name = pd.read_csv('./Wikidata_queries/naming_lists/ethnic_group_names.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to put the data together right away and drop the duplicates and then check if there are any wikidata ID's that have several names attached which should not happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['item', 'itemLabel'], dtype='object')\n",
      "Index(['item', 'itemLabel'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "act_csvs = [actor_name_M, actor_name_F, film_actor_name, tv_actor_name, voice_actor_name]\n",
    "dir_csvs = [dir_name, film_dir_name, swriter_name]\n",
    "\n",
    "actor_name_map = pd.concat(act_csvs)\n",
    "dir_name_map = pd.concat(dir_csvs)\n",
    "\n",
    "print(actor_name_map.columns)\n",
    "print(dir_name_map.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the actor name map is 352555, and the number of unique values is 352555\n",
      "The length of the director name map is 179643, and the number of unique values is 179643\n"
     ]
    }
   ],
   "source": [
    "actor_name_map.drop_duplicates(keep='first', inplace=True)\n",
    "dir_name_map.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "print(f'The length of the actor name map is {len(actor_name_map)}, and the number of unique values is {actor_name_map.item.nunique()}')\n",
    "print(f'The length of the director name map is {len(dir_name_map)}, and the number of unique values is {dir_name_map.item.nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good from there and now let's do the same for the ethnicity name map and the gender name map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the gender name map is 1402, and the number of unique values is 1402\n",
      "The length of the ethnicity name map is 16412, and the number of unique values is 16412\n"
     ]
    }
   ],
   "source": [
    "ethn_name_map = ethn_name.copy()\n",
    "gender_name_map = gender_name.copy()\n",
    "\n",
    "ethn_name_map.drop_duplicates(keep='first', inplace=True)\n",
    "gender_name_map.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "print(f'The length of the gender name map is {len(gender_name_map)}, and the number of unique values is {gender_name_map.item.nunique()}')\n",
    "print(f'The length of the ethnicity name map is {len(ethn_name_map)}, and the number of unique values is {ethn_name_map.item.nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start to join all of the dataframe together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['actor', 'gender_wikiID', 'dob', 'height', 'ethnic_group', 'name',\n",
      "       'ethnicity', 'gender'],\n",
      "      dtype='object')\n",
      "Index(['dir_swriter', 'gender_wikiID', 'dob', 'height', 'ethnic_group', 'name',\n",
      "       'ethnicity', 'gender'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def replace_col (df, old_names, new_names):\n",
    "    assert len(old_names) == len(new_names),'There lists are not of the same length'\n",
    "    for i in range(len(old_names)):\n",
    "        df.rename(columns={old_names[i]:new_names[i]}, inplace=True)\n",
    "\n",
    "\n",
    "all_actors_temp1 = all_actors_clean.merge(actor_name_map, how='left', left_on='actor', right_on='item')\n",
    "all_dir_temp1 = all_dir_clean.merge(dir_name_map, how='left', left_on='dir_swriter', right_on='item')\n",
    "all_actors_temp1.drop('item', axis=1, inplace=True)\n",
    "replace_col(all_actors_temp1, ['itemLabel'], ['name'])\n",
    "all_dir_temp1.drop('item', axis=1, inplace=True)\n",
    "replace_col(all_dir_temp1, ['itemLabel'], ['name'])\n",
    "\n",
    "\n",
    "all_actors_temp2 = all_actors_temp1.merge(ethn_name_map, how='left', left_on='ethnic_group', right_on='item')\n",
    "all_dir_temp2 = all_dir_temp1.merge(ethn_name_map, how='left', left_on='ethnic_group', right_on='item')\n",
    "all_actors_temp2.drop('item', axis=1, inplace=True)\n",
    "replace_col(all_actors_temp2, ['itemLabel'], ['ethnicity'])\n",
    "all_dir_temp2.drop('item', axis=1, inplace=True)\n",
    "replace_col(all_dir_temp2, ['itemLabel'], ['ethnicity'])\n",
    "\n",
    "\n",
    "replace_col(all_actors_temp2,['gender'], ['gender_wikiID'])\n",
    "replace_col(all_dir_temp2,['gender'], ['gender_wikiID'])\n",
    "\n",
    "all_actors_temp3 = all_actors_temp2.merge(gender_name_map, how='left', left_on='gender_wikiID', right_on='item')\n",
    "all_dir_temp3 = all_dir_temp2.merge(gender_name_map, how='left', left_on='gender_wikiID', right_on='item')\n",
    "all_actors_temp3.drop('item', axis=1, inplace=True)\n",
    "replace_col(all_actors_temp3, ['itemLabel'], ['gender'])\n",
    "all_dir_temp3.drop('item', axis=1, inplace=True)\n",
    "replace_col(all_dir_temp3, ['itemLabel'], ['gender'])\n",
    "\n",
    "print(all_actors_temp3.columns)\n",
    "print(all_dir_temp3.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can drop the columns that are not useful and rearrange them to get the final Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['wikiDataID', 'birth', 'height', 'name', 'ethnicity', 'gender'], dtype='object')\n",
      "Index(['wikiDataID', 'birth', 'height', 'name', 'ethnicity', 'gender'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "all_actors_temp3.drop('gender_wikiID', axis=1, inplace=True)\n",
    "all_dir_temp3.drop('gender_wikiID', axis=1, inplace=True)\n",
    "all_actors_temp3.drop('ethnic_group', axis=1, inplace=True)\n",
    "all_dir_temp3.drop('ethnic_group', axis=1, inplace=True)\n",
    "replace_col(all_actors_temp3, ['actor', 'dob'], ['wikiDataID', 'birth'])\n",
    "replace_col(all_dir_temp3, ['dir_swriter', 'dob'], ['wikiDataID', 'birth'])\n",
    "print(all_actors_temp3.columns)\n",
    "print(all_dir_temp3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_final = pd.DataFrame()\n",
    "dir_final = pd.DataFrame()\n",
    "\n",
    "actors_final[[\n",
    "    'name',\n",
    "    'gender',\n",
    "    'birth',\n",
    "    'height',\n",
    "    'ethnicity',\n",
    "    'wikidata_id']] = all_actors_temp3[['name', 'gender', 'birth', 'height', 'ethnicity', 'wikiDataID']]\n",
    "\n",
    "dir_final[[\n",
    "    'name',\n",
    "    'gender',\n",
    "    'birth',\n",
    "    'height',\n",
    "    'ethnicity',\n",
    "    'wikidata_id']] = all_dir_temp3[['name', 'gender', 'birth', 'height', 'ethnicity', 'wikiDataID']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's clean the birth dates as well as the wikidataIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_final.birth = actors_final[actors_final.birth.notna()].birth.apply(lambda x: x.split('T')[0])\n",
    "dir_final.birth = dir_final[dir_final.birth.notna()].birth.apply(lambda x: x.split('T')[0])\n",
    "\n",
    "actors_final.birth = actors_final[actors_final.birth.notna()].birth.apply(lambda x: np.nan if x.find('.org') != -1 else x)\n",
    "dir_final.birth = dir_final[dir_final.birth.notna()].birth.apply(lambda x: np.nan if x.find('.org') != -1 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are fictional characters in here as well as actor's with birthdays that are not actual days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>birth</th>\n",
       "      <th>height</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>wikidata_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Henk Rigters</td>\n",
       "      <td>male</td>\n",
       "      <td>1915-10-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.wikidata.org/entity/Q100001260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Florian Jahr</td>\n",
       "      <td>male</td>\n",
       "      <td>1983-06-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.wikidata.org/entity/Q1000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ulrike Scheel</td>\n",
       "      <td>female</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.wikidata.org/entity/Q1000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nadia Boule</td>\n",
       "      <td>female</td>\n",
       "      <td>1984-10-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.wikidata.org/entity/Q100011609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Renee Goddard</td>\n",
       "      <td>female</td>\n",
       "      <td>1923-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.wikidata.org/entity/Q1000204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386663</th>\n",
       "      <td>Eleonora Bolla</td>\n",
       "      <td>female</td>\n",
       "      <td>1986-12-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.wikidata.org/entity/Q99997567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386664</th>\n",
       "      <td>Lison Daniel</td>\n",
       "      <td>female</td>\n",
       "      <td>1992-06-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.wikidata.org/entity/Q99997711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386665</th>\n",
       "      <td>Giovanni Maria Buzzatti</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.wikidata.org/entity/Q99997849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386666</th>\n",
       "      <td>Gabriella Infelise</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.wikidata.org/entity/Q99998028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386667</th>\n",
       "      <td>Valentina Reggio</td>\n",
       "      <td>female</td>\n",
       "      <td>1991-08-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.wikidata.org/entity/Q99998088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>386668 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name  gender       birth height ethnicity  \\\n",
       "0                  Henk Rigters    male  1915-10-06    NaN       NaN   \n",
       "1                  Florian Jahr    male  1983-06-23    NaN       NaN   \n",
       "2                 Ulrike Scheel  female  2000-01-01    NaN       NaN   \n",
       "3                   Nadia Boule  female  1984-10-01    NaN       NaN   \n",
       "4                 Renee Goddard  female  1923-01-01    NaN       NaN   \n",
       "...                         ...     ...         ...    ...       ...   \n",
       "386663           Eleonora Bolla  female  1986-12-22    NaN       NaN   \n",
       "386664             Lison Daniel  female  1992-06-12    NaN       NaN   \n",
       "386665  Giovanni Maria Buzzatti    male         NaN    NaN       NaN   \n",
       "386666       Gabriella Infelise  female         NaN    NaN       NaN   \n",
       "386667         Valentina Reggio  female  1991-08-15    NaN       NaN   \n",
       "\n",
       "                                      wikidata_id  \n",
       "0       http://www.wikidata.org/entity/Q100001260  \n",
       "1         http://www.wikidata.org/entity/Q1000015  \n",
       "2         http://www.wikidata.org/entity/Q1000089  \n",
       "3       http://www.wikidata.org/entity/Q100011609  \n",
       "4         http://www.wikidata.org/entity/Q1000204  \n",
       "...                                           ...  \n",
       "386663   http://www.wikidata.org/entity/Q99997567  \n",
       "386664   http://www.wikidata.org/entity/Q99997711  \n",
       "386665   http://www.wikidata.org/entity/Q99997849  \n",
       "386666   http://www.wikidata.org/entity/Q99998028  \n",
       "386667   http://www.wikidata.org/entity/Q99998088  \n",
       "\n",
       "[386668 rows x 6 columns]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(actors_final[actors_final['birth'].notna()].birth, format=r\"%Y-%m-%d\", errors='coerce')\n",
    "pd.to_datetime(dir_final[dir_final.birth.notna()].birth, format=r\"%Y-%m-%d\", errors='coerce')\n",
    "\n",
    "actors_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_final['wikidata_id'] = actors_final['wikidata_id'].apply(lambda x: x.split('/')[-1])\n",
    "dir_final['wikidata_id'] = dir_final['wikidata_id'].apply(lambda x: x.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay some final numbers before we part ways with this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>birth</th>\n",
       "      <th>height</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>wikidata_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>344814</td>\n",
       "      <td>378155</td>\n",
       "      <td>285949</td>\n",
       "      <td>16617.0</td>\n",
       "      <td>1255</td>\n",
       "      <td>386668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>338989</td>\n",
       "      <td>2</td>\n",
       "      <td>51436</td>\n",
       "      <td>436.0</td>\n",
       "      <td>260</td>\n",
       "      <td>386668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Nana</td>\n",
       "      <td>male</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>160.0</td>\n",
       "      <td>African Americans</td>\n",
       "      <td>Q100001260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>211122</td>\n",
       "      <td>2508</td>\n",
       "      <td>964.0</td>\n",
       "      <td>297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name  gender       birth   height          ethnicity wikidata_id\n",
       "count   344814  378155      285949  16617.0               1255      386668\n",
       "unique  338989       2       51436    436.0                260      386668\n",
       "top       Nana    male  2000-01-01    160.0  African Americans  Q100001260\n",
       "freq         5  211122        2508    964.0                297           1"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actors_final.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>birth</th>\n",
       "      <th>height</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>wikidata_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>174307</td>\n",
       "      <td>164321</td>\n",
       "      <td>118734</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>644</td>\n",
       "      <td>174323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>171503</td>\n",
       "      <td>2</td>\n",
       "      <td>35534</td>\n",
       "      <td>148.0</td>\n",
       "      <td>183</td>\n",
       "      <td>174323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Jan Novák</td>\n",
       "      <td>male</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>170.0</td>\n",
       "      <td>African Americans</td>\n",
       "      <td>Q1000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>128310</td>\n",
       "      <td>1739</td>\n",
       "      <td>62.0</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  gender       birth  height          ethnicity wikidata_id\n",
       "count      174307  164321      118734  1102.0                644      174323\n",
       "unique     171503       2       35534   148.0                183      174323\n",
       "top     Jan Novák    male  2000-01-01   170.0  African Americans    Q1000002\n",
       "freq            5  128310        1739    62.0                155           1"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_final.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put this away for now and save it to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_final.to_csv('./Wikidata_Actors.csv')\n",
    "dir_final.to_csv('./Wikidata_Directors.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffe78d3011582e93732d41361e262d52dfd2e5fd6fc55e676e876753b981662d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
